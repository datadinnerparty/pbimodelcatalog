{"cells":[{"cell_type":"code","source":["#import necessary libraries\n","import sempy.fabric as fabric\n","import pandas as pd\n","import numpy as np\n","import re\n","from notebookutils import mssparkutils\n","\n","# set temporary config for date issue\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n","\n","##########################################################################################################\n","##                    THIS IS THE SECTION THAT REQUIRES INITIAL CONFIGURATION BY THE USER\n","##########################################################################################################\n","########################################################################################\n","###   MANUAL CONFIG HERE\n","#######################################################################################\n","#Define lakehouse and schema names\n","workspace_name = \"amaser_dp700\" #this is the workspace name where the notebook runs, and where the lakehouse lives\n","lakehouse_name = \"testthislakehouseoption\"\n","lakehouse_description = \"Model Documentation Lakehouse\"  # this is only used if the notebook creates a new lakehouse\n","schema_name = \"docuschema\"  # leave this blank if using the built in dbo schema, or NOT using the schema enabled lakehouse\n","\n","\n","# Define colors for table navigator page\n","factcolor = '#A6B916'\n","dimcolor = '#C8B78A'\n","defaultcolor = '#A66999'\n","\n","#Create list of workspaces and Models.  This allows users to limit which models get documented.  A user could also\n","#   tweak this notebook, to use sempy functions, and query all workspaces/models that he/she has access to, if that's preferred.\n","#THE ORDER OF THE WORKSPACES AND MODELS MUST BE THE SAME\n","#  ie the third report MUST be in the third worksapce\n","data = {\n","    'Workspace': ['Industry Demos', 'Industry Demos','PTurley AzureDevOps Integration'],\n","    'Model': ['Insurance Power BI Demo', 'Product Model','Airline Performance']\n","}\n","###########################################################################################\n","workspace_id = fabric.get_workspace_id()\n","\n","#######################################################\n","## Check for existing items\n","######################################################\n","## check if lakehouse exists\n","try:\n","    lakehouse_object = mssparkutils.lakehouse.get(lakehouse_name)\n","    lakehouse_id=lakehouse_object.id\n","except Exception as e:\n","    try:\n","\n","        fabric.create_lakehouse(display_name=lakehouse_name,description=lakehouse_description,workspace=workspace_name)\n","        lakehouse_object = mssparkutils.lakehouse.get(lakehouse_name)\n","        lakehouse_id=lakehouse_object.id\n","    except Exception as e:\n","        print('Unable to create lakehouse')\n","        print(e)\n","\n","\n","\n","##create schema if not exists\n","if schema_name == \"\":\n","    lakehouse_value = f'{lakehouse_name}.'\n","else:\n","    #if fabric.lakehouse.schema_exists(schema_name,lakehouse=lakehouse_id,workspace=workspace_id):\n","    #    lakehouse_value = f'{workspace_name}.{lakehouse_name}.{schema_name}.'\n","    #else:\n","       \n","            #schema_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Tables/{schema_name}\"\n","            #mssparkutils.fs.mkdirs(schema_path)\n","            schemaquery = f\"CREATE SCHEMA IF NOT EXISTS {workspace_name}.{lakehouse_name}.{schema_name} \"\n","            spark.sql(schemaquery)\n","            lakehouse_value = f'{workspace_name}.{lakehouse_name}.{schema_name}.'\n","        \n","\n","print(lakehouse_value)\n","\n","#create the log table. \n","logquery = f'CREATE TABLE IF NOT EXISTS {lakehouse_value}modelLog (timeval TIMESTAMP, table_name STRING, Message STRING) USING DELTA'\n","#print(logquery)\n","spark.sql(logquery)\n","\n","#function to log activity.  This function can be tweaked to add lakehouse/schema name, so all the code in the lower cells don't need to be adjusted\n","def logActivity(table_name,message):\n","    logquery = f'INSERT INTO {lakehouse_value}modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"{table_name}\",\"{message}\")'\n","    #print(f'log query={logquery}')\n","    #print(logquery)\n","    spark.sql(logquery)\n","\n","#function to write the dataframe as a database\n","def writeTable(pd_df, tablename):\n","  if len(pd_df) != 0:\n","    query = f\"drop table if exists {lakehouse_value}{tablename}\"\n","    spark.sql(query)\n","    spark_df = spark.createDataFrame(pd_df)\n","    full_table_name = f'{lakehouse_value}{tablename}'\n","    #print(full_table_name)\n","    try:\n","        spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").saveAsTable(full_table_name)\n","        logActivity(tablename,f'{tablename} written to lakehouse')\n","    except Exception as e:\n","        logActivity(tablename,f'Failed to write {tablename}')\n","        \n","\n","####note\n","# You must attach the requested lakehouse to this notebook, for it to work properly.\n","#     If you are using schema enabled lakehousse, you'll want to modify the writeTable function to write to f'{lakehouse}.{schema}.{tablename}'abs\n","#      to account for those options. You'll need to update the query variable as well\n","\n","#######################################################################################################\n","## from this section onward, there shouldn't be any updating necessary\n","#######################################################################################################\n","# add in the path column.  this isn't strictly necessary\n","df = pd.DataFrame(data)\n","df['WorkspacePath'] = f'powerbi://api.powerbi.com/v1.0/myorg/'+ df['Workspace']\n","df['semanticmodel_id'] = df.index + 1 # addsan auto increment index to  the table.\n","logActivity('SemanticModel','SemanticModel processing started')\n","\n","\n","####################################################################################################################\n","\n","\n","#DEFINE FUNCTIONS\n","\n","#function to get data from model\n","def getMetaData(daxquery,fieldname,indexfield):\n","    indexname = indexfield+'_id'\n","    #build out first row\n","    model_df = fabric.evaluate_dax(\n","        workspace=df.iloc[0,0],\n","        dataset=df.iloc[0,1],\n","        dax_string=daxquery\n","    )\n","    model_df['semanticmodel_id'] = df.iloc[0,3]\n","    model_df['SemanticModel'] = df.iloc[0,1]\n","    #cycle through remaining rows, if more than 1 row\n","    if len(df) > 1:\n","        for index,row in df.iloc[1:].iterrows():\n","            temp_df = fabric.evaluate_dax(\n","                workspace=df.iloc[index,0],\n","                dataset=df.iloc[index,1],\n","                dax_string=dax_query\n","            )\n","            temp_df['semanticmodel_id']=df.iloc[index,3]\n","            temp_df['SemanticModel'] = df.iloc[index,1]\n","            #print('temp df executed')\n","            #append most recent\n","            model_df = pd.concat([model_df,temp_df],ignore_index=True)\n","\n","    model_df.columns = model_df.columns.str.replace('[', '')\n","    model_df.columns = model_df.columns.str.replace(']', '')\n","    model_df = model_df.rename(columns={'Name':fieldname})\n","    model_df[indexname] = model_df.index + 1\n","    return model_df\n","\n","\n","# Function to extract hashtags from a string\n","def extract_hashtags(description):\n","    if isinstance(description, str):  # Check if description is a string\n","        hashtags = re.findall(r'%%(\\w+)', description)\n","        return hashtags\n","    else:\n","        return []\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"7f293312-2244-4a98-b187-e4258ae663d2","normalized_state":"finished","queued_time":"2025-11-26T19:42:54.0305434Z","session_start_time":null,"execution_start_time":"2025-11-26T19:42:54.031868Z","execution_finish_time":"2025-11-26T19:42:57.8833939Z","parent_msg_id":"b206d052-7653-4903-92bc-ae00317d3c81"},"text/plain":"StatementMeta(, 7f293312-2244-4a98-b187-e4258ae663d2, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["amaser_dp700.testthislakehouseoption.docuschema.\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"SemanticModel\",\"SemanticModel processing started\")\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ef28b2d-c188-4f5e-98c6-5dd1fb680be1"},{"cell_type":"markdown","source":["Process list of semantic models from the user-defined list. save as a table in the lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6b6e463c-50f6-4899-b592-deb0d7b667d8"},{"cell_type":"code","source":["#Write Semantic Model table\n","writeTable(df,\"SemanticModel\")\n","logActivity('SemanticModel','SemanticModel processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"7f293312-2244-4a98-b187-e4258ae663d2","normalized_state":"finished","queued_time":"2025-11-26T19:43:02.1311796Z","session_start_time":null,"execution_start_time":"2025-11-26T19:43:02.1323708Z","execution_finish_time":"2025-11-26T19:43:14.9403419Z","parent_msg_id":"a3a5e3dc-b19a-4859-9be0-392b60a10d59"},"text/plain":"StatementMeta(, 7f293312-2244-4a98-b187-e4258ae663d2, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["amaser_dp700.testthislakehouseoption.docuschema.SemanticModel\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"SemanticModel\",\"SemanticModel written to lakehouse\")\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"SemanticModel\",\"SemanticModel processing completed\")\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"45b3f910-f26b-4ea8-a6d8-2ae8b6c29f1c"},{"cell_type":"markdown","source":["Process Model query.  Save Model data to the lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"04fff423-ad40-4ded-95dd-eb378d63a148"},{"cell_type":"code","source":["#Get Model Data\n","logActivity('Model','Model processing started')\n","dax_query = \"EVALUATE INFO.MODEL()\"\n","model_df = getMetaData(dax_query,'ModelName','model')\n","#display(model_df)\n","\n","writeTable(model_df,\"Model\")\n","logActivity('Model','Model processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"7f293312-2244-4a98-b187-e4258ae663d2","normalized_state":"finished","queued_time":"2025-11-26T19:43:22.3251417Z","session_start_time":null,"execution_start_time":"2025-11-26T19:43:22.3262716Z","execution_finish_time":"2025-11-26T19:44:23.0836038Z","parent_msg_id":"960cbf5b-c106-4590-b121-883833f678c4"},"text/plain":"StatementMeta(, 7f293312-2244-4a98-b187-e4258ae663d2, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Model\",\"Model processing started\")\n"]},{"output_type":"stream","name":"stdout","text":["amaser_dp700.testthislakehouseoption.docuschema.Model\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Model\",\"Model written to lakehouse\")\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Model\",\"Model processing completed\")\n"]}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"3b5072d7-6ed2-4097-8cb5-b5f46a3f6369"},{"cell_type":"markdown","source":["Process Tables dax query to collect table data from all defined models"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"03b0cb49-c861-4dfb-b7a8-01c421a234b3"},{"cell_type":"code","source":["#get Table Data\n","logActivity('PBITables','Table processing started')\n","dax_query = \"\"\"EVALUATE\n","VAR __tables =\n","    INFO.TABLES ()\n","VAR __storagemode =\n","    SELECTCOLUMNS ( INFO.VIEW.TABLES (), \\\"ID\\\", [ID], \\\"StorageMode\\\", [StorageMode] )\n","VAR __combined =\n","    NATURALLEFTOUTERJOIN ( __tables, __storagemode )\n","RETURN\n","    __combined\n","\n","\"\"\"\n","table_df = getMetaData(dax_query,'TableName','table')\n","\n","#add table type column for slicer. Cleanup column names and create combined key for joining\n","table_df['TableType']=table_df['Description'].apply(lambda x:extract_hashtags(x))\n","table_df['TableType']=table_df['TableType'].astype(str)\n","table_df['TableType'] = table_df['TableType'].str.removeprefix('[\\'')\n","table_df['TableType'] = table_df['TableType'].str.replace('\\']', '')\n","table_df['TableType'] = table_df['TableType'].str.replace(']', '')\n","table_df['TableType'] = table_df['TableType'].str.replace('[', '')\n","table_df['compoundkey'] = table_df['SemanticModel'] + table_df['ID'].astype(str)\n","\n","#display(table_df)\n","writeTable(table_df,\"PBITables\")\n","logActivity('PBITables','Table processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"7f293312-2244-4a98-b187-e4258ae663d2","normalized_state":"finished","queued_time":"2025-11-26T19:43:22.4032841Z","session_start_time":null,"execution_start_time":"2025-11-26T19:44:23.086022Z","execution_finish_time":"2025-11-26T19:44:37.6593382Z","parent_msg_id":"43e8bd61-49e5-4ffe-9488-de745c2bd046"},"text/plain":"StatementMeta(, 7f293312-2244-4a98-b187-e4258ae663d2, 13, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"PBITables\",\"Table processing started\")\n"]},{"output_type":"stream","name":"stdout","text":["amaser_dp700.testthislakehouseoption.docuschema.PBITables\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"PBITables\",\"PBITables written to lakehouse\")\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"PBITables\",\"Table processing completed\")\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"aa933b1d-5b64-408b-a671-c1279d629ad8"},{"cell_type":"markdown","source":["collect measure data from listed models and store in the lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"86a63b87-c467-4448-87f1-64686e26facd"},{"cell_type":"code","source":["#get measure Data\n","logActivity('MeasureList','MeasureList processing started')\n","dax_query = \"EVALUATE INFO.Measures()\"\n","measure_df = getMetaData(dax_query,'MeasureName','measures')\n","\n","#display(measure_df)\n","\n","writeTable(measure_df,\"MeasureList\")\n","logActivity('MeasureList','MeasureList processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"7f293312-2244-4a98-b187-e4258ae663d2","normalized_state":"finished","queued_time":"2025-11-26T19:43:22.4839648Z","session_start_time":null,"execution_start_time":"2025-11-26T19:44:37.6705214Z","execution_finish_time":"2025-11-26T19:44:54.6424411Z","parent_msg_id":"8a0341d7-937c-4735-9850-3bf220f536ad"},"text/plain":"StatementMeta(, 7f293312-2244-4a98-b187-e4258ae663d2, 14, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"MeasureList\",\"MeasureList processing started\")\n"]},{"output_type":"stream","name":"stdout","text":["amaser_dp700.testthislakehouseoption.docuschema.MeasureList\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"MeasureList\",\"MeasureList written to lakehouse\")\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"MeasureList\",\"MeasureList processing completed\")\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f4a2e9d8-f20c-49b4-8450-de7bae4505ef"},{"cell_type":"markdown","source":["\n","get role data (if any) from all selected models, and store it in the lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6e1d1ef0-bd78-493f-8b72-81ae2f3567b5"},{"cell_type":"code","source":["#get role Data\n","logActivity('PBIRoles','Role processing started')\n","dax_query = \"EVALUATE INFO.Roles()\"\n","role_df = getMetaData(dax_query,'RoleName','roles')\n","\n","#display(role_df)\n","\n","writeTable(role_df,\"PBIRoles\")\n","logActivity('PBIRoles','Role processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"7f293312-2244-4a98-b187-e4258ae663d2","normalized_state":"finished","queued_time":"2025-11-26T19:43:22.652544Z","session_start_time":null,"execution_start_time":"2025-11-26T19:44:54.6449273Z","execution_finish_time":"2025-11-26T19:45:18.5662005Z","parent_msg_id":"a9d79b92-92ce-4170-8884-811b355fc403"},"text/plain":"StatementMeta(, 7f293312-2244-4a98-b187-e4258ae663d2, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"PBIRoles\",\"Role processing started\")\n"]},{"output_type":"stream","name":"stdout","text":["amaser_dp700.testthislakehouseoption.docuschema.PBIRoles\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"PBIRoles\",\"PBIRoles written to lakehouse\")\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"PBIRoles\",\"Role processing completed\")\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"20b8978a-11fc-404a-b97e-c3c1abb9b9b5"},{"cell_type":"markdown","source":["get data about any partitions.  This may only be important in regards to incremental refresh, but either way, the data is available in the documentation lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0f6484c2-e4e4-4b68-bc7b-b52836e5a31d"},{"cell_type":"code","source":["#get partition Data\n","logActivity('Partitions','Partition processing started')\n","dax_query = \"\"\"\n","EVALUATE\n","VAR __partitions =\n","    INFO.PARTITIONS ()\n","VAR __refreshpolicies =\n","    SELECTCOLUMNS (\n","        INFO.REFRESHPOLICIES (),\n","        \\\"TableID\\\", [TableID],\n","        \\\"SourceExpression\\\", [SourceExpression]\n","    )\n","VAR __combined =\n","    NATURALLEFTOUTERJOIN ( __partitions, __refreshpolicies )\n","RETURN\n","    __combined\n","\"\"\"\n","partition_df = getMetaData(dax_query,'PartitionName','partitions')\n","\n","#add extra columns for joins, and cleanup unused columns\n","partition_df['compoundkey'] = partition_df['SemanticModel'] + partition_df['TableID'].astype(str)\n","partition_df =pd.merge(partition_df,table_df[['compoundkey','TableName']],left_on='compoundkey',right_on='compoundkey',how='left')\n","partition_df['Source'] = np.where(partition_df['QueryDefinition'].isnull(),partition_df['SourceExpression'],partition_df['QueryDefinition'])\n","partition_df = partition_df.drop(['QueryDefinition','SourceExpression'],axis=1)\n","#display(partition_df)\n","\n","writeTable(partition_df,\"Partitions\")\n","logActivity('Partitions','Partition processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"7f293312-2244-4a98-b187-e4258ae663d2","normalized_state":"finished","queued_time":"2025-11-26T19:43:22.7334158Z","session_start_time":null,"execution_start_time":"2025-11-26T19:45:18.5685822Z","execution_finish_time":"2025-11-26T19:45:40.2126715Z","parent_msg_id":"764e92f1-ec71-4a35-bb30-0c63c97885ad"},"text/plain":"StatementMeta(, 7f293312-2244-4a98-b187-e4258ae663d2, 16, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Partitions\",\"Partition processing started\")\n"]},{"output_type":"stream","name":"stdout","text":["amaser_dp700.testthislakehouseoption.docuschema.Partitions\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Partitions\",\"Partitions written to lakehouse\")\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Partitions\",\"Partition processing completed\")\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"82882fbe-a690-4440-9c8c-c89854bb0072"},{"cell_type":"markdown","source":["get expression data.  This includes data sources (as shown in TE) and any parameters that may be defined."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a72cb4aa-58f6-4ce6-a471-55b7fcc1d139"},{"cell_type":"code","source":["#get expression Data\n","logActivity('Expression','Expression processing started')\n","dax_query = \"EVALUATE INFO.Expressions()\"\n","expression_df = getMetaData(dax_query,'ExpressionName','expression')\n","\n","#display(expression_df)\n","\n","writeTable(expression_df,\"Expression\")\n","logActivity('Expression','Expression processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"7f293312-2244-4a98-b187-e4258ae663d2","normalized_state":"finished","queued_time":"2025-11-26T19:43:22.8054722Z","session_start_time":null,"execution_start_time":"2025-11-26T19:45:40.2151323Z","execution_finish_time":"2025-11-26T19:45:57.3268591Z","parent_msg_id":"434a255f-1718-418d-9f0b-764499fca5d0"},"text/plain":"StatementMeta(, 7f293312-2244-4a98-b187-e4258ae663d2, 17, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Expression\",\"Expression processing started\")\n"]},{"output_type":"stream","name":"stdout","text":["amaser_dp700.testthislakehouseoption.docuschema.Expression\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Expression\",\"Expression written to lakehouse\")\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Expression\",\"Expression processing completed\")\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b143325d-b5e3-4f54-a62c-a88ba4116204"},{"cell_type":"markdown","source":["get hierarchy data (if any)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4a32ab93-d97e-41c0-a608-2ecca7bc98b9"},{"cell_type":"code","source":["#get hierarchy Data\n","logActivity('Hierarchies','Hierarchy processing started')\n","dax_query = \"EVALUATE INFO.Hierarchies()\"\n","hierarchy_df = getMetaData(dax_query,'HierarchyName','hierarchy')\n","\n","#display(final_df)\n","#add colulmn for join with table\n","hierarchy_df['compoundkey'] = hierarchy_df['SemanticModel'] + hierarchy_df['TableID'].astype(str)\n","\n","hierarchy_df =pd.merge(hierarchy_df,table_df[['compoundkey','TableName']],left_on='compoundkey',right_on='compoundkey',how='left')\n","#display(hierarchy_df)\n","\n","writeTable(hierarchy_df,\"Hierarchies\")\n","logActivity('Hierarchies','Hierarchy processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"7f293312-2244-4a98-b187-e4258ae663d2","normalized_state":"finished","queued_time":"2025-11-26T19:43:22.8639329Z","session_start_time":null,"execution_start_time":"2025-11-26T19:45:57.3292342Z","execution_finish_time":"2025-11-26T19:46:12.2313725Z","parent_msg_id":"96cfe42d-1655-4bcf-b49f-7820594ae667"},"text/plain":"StatementMeta(, 7f293312-2244-4a98-b187-e4258ae663d2, 18, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Hierarchies\",\"Hierarchy processing started\")\n"]},{"output_type":"stream","name":"stdout","text":["amaser_dp700.testthislakehouseoption.docuschema.Hierarchies\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Hierarchies\",\"Hierarchies written to lakehouse\")\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Hierarchies\",\"Hierarchy processing completed\")\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8885ae53-93fb-4e70-ac87-e4cb89d0adb2"},{"cell_type":"markdown","source":["get column data from selected models"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e234bc3-a699-4187-a670-abc234001587"},{"cell_type":"code","source":["#get column Data\n","logActivity('PBIColumns','Column processing started')\n","dax_query = \"EVALUATE INFO.Columns()\"\n","columns_df = getMetaData(dax_query,'ColumnName','column')\n","\n","#add compound key for table join.  add colulmn to determine calculated columns (used for slicer)\n","columns_df['compoundkey'] = columns_df['SemanticModel'] + columns_df['TableID'].astype(str)\n","columns_df =pd.merge(columns_df,table_df[['compoundkey','TableName']],left_on='compoundkey',right_on='compoundkey',how='left')\n","columns_df['ColumnName'] = columns_df['ExplicitName'].combine_first(columns_df['InferredName'])\n","del columns_df['ExplicitName']\n","del columns_df['InferredName']\n","columns_df['ColumnType'] = np.where(columns_df['Expression'].isnull(),'Standard','Calculated')\n","#display(columns_df)\n","\n","writeTable(columns_df,\"PBIColumns\")\n","logActivity('PBIColumns','Column processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"7f293312-2244-4a98-b187-e4258ae663d2","normalized_state":"finished","queued_time":"2025-11-26T19:43:22.9704593Z","session_start_time":null,"execution_start_time":"2025-11-26T19:46:12.233816Z","execution_finish_time":"2025-11-26T19:46:29.411839Z","parent_msg_id":"c13c6f2e-5b50-45a9-ab42-79d316c00721"},"text/plain":"StatementMeta(, 7f293312-2244-4a98-b187-e4258ae663d2, 19, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"PBIColumns\",\"Column processing started\")\n"]},{"output_type":"stream","name":"stdout","text":["amaser_dp700.testthislakehouseoption.docuschema.PBIColumns\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"PBIColumns\",\"PBIColumns written to lakehouse\")\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"PBIColumns\",\"Column processing completed\")\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a9f7e173-a3ac-4188-a82c-80e2a7281ddb"},{"cell_type":"markdown","source":["get info on any calculation groups from selected models"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ba0eff92-dc61-478f-8d24-ac14ac50285e"},{"cell_type":"code","source":["#get calculation groups\n","logActivity('CalculationItems','Calculation Item processing started')\n","dax_query = \"EVALUATE INFO.CalculationGroups()\"\n","calcgroup_df = getMetaData(dax_query,'CalculationGroupName','calcgroup')\n","calcgroup_df['compoundkey'] = calcgroup_df['SemanticModel'] + calcgroup_df['TableID'].astype(str)\n","calcgroup_df['groupcompoundkey'] = calcgroup_df['SemanticModel'] + calcgroup_df['ID'].astype(str)\n","calcgroup_df =pd.merge(calcgroup_df,table_df[['compoundkey','TableName']],left_on='compoundkey',right_on='compoundkey',how='left')\n","calcgroup_df['CalculationGroupName'] = calcgroup_df['TableName']\n","#display(calcgroup_df)\n","\n","#get calcitems\n","dax_query = \"EVALUATE INFO.CalculationItems()\"\n","items_df = getMetaData(dax_query,'CalculationItemName','calcitem')\n","items_df['compoundkey'] = items_df['SemanticModel'] + items_df['CalculationGroupID'].astype(str)\n","items_df = pd.merge(items_df,calcgroup_df[['groupcompoundkey','CalculationGroupName']],left_on='compoundkey',right_on='groupcompoundkey',how='left')\n","#display(items_df)\n","\n","writeTable(items_df,\"CalculationItems\")\n","logActivity('CalculationItems','Calculation Item processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"7f293312-2244-4a98-b187-e4258ae663d2","normalized_state":"finished","queued_time":"2025-11-26T19:43:23.0407343Z","session_start_time":null,"execution_start_time":"2025-11-26T19:46:29.4141913Z","execution_finish_time":"2025-11-26T19:46:46.5198561Z","parent_msg_id":"f94db054-463c-40fa-a29b-4cf892a9deb1"},"text/plain":"StatementMeta(, 7f293312-2244-4a98-b187-e4258ae663d2, 20, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"CalculationItems\",\"Calculation Item processing started\")\n"]},{"output_type":"stream","name":"stdout","text":["amaser_dp700.testthislakehouseoption.docuschema.CalculationItems\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"CalculationItems\",\"CalculationItems written to lakehouse\")\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"CalculationItems\",\"Calculation Item processing completed\")\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e09de0ce-0815-4ccb-a341-6058dfd6da87"},{"cell_type":"markdown","source":["get Relationship data from selected models.  this does not include relationships for this documentation model "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9472f87c-5bcf-42a9-ade9-8e8a4c050fb3"},{"cell_type":"code","source":["#get relationships\n","logActivity('Relationships','Relationship processing started')\n","dax_query = \"EVALUATE INFO.Relationships()\"\n","relationship_df = getMetaData(dax_query,'RelationshipName','relationship')\n","# clean up colulmns for to/from relationships.  \n","relationship_df['FromCompoundKey'] = relationship_df['SemanticModel'] + relationship_df['FromTableID'].astype(str)\n","relationship_df['ToCompoundKey'] = relationship_df['SemanticModel'] + relationship_df['ToTableID'].astype(str)\n","relationship_df =pd.merge(relationship_df,table_df[['compoundkey','TableName','TableType']],left_on='FromCompoundKey',right_on='compoundkey',how='left')\n","relationship_df = relationship_df.rename(columns={'TableName':'FromTableName','TableType':'FromTableType'})\n","relationship_df =pd.merge(relationship_df,table_df[['compoundkey','TableName','TableType']],left_on='ToCompoundKey',right_on='compoundkey',how='left')\n","relationship_df = relationship_df.rename(columns={'TableName':'ToTableName','TableType':'ToTableType'})\n","relationship_df['FromCardinality'] = relationship_df['FromCardinality'].astype(str)\n","relationship_df['FromCardinality'] = relationship_df['FromCardinality'].replace('2','Many')\n","relationship_df['ToCardinality'] = relationship_df['ToCardinality'].astype(str)\n","relationship_df['ToCardinality'] = relationship_df['ToCardinality'].replace('2','Many')\n","####################################################################################################################\n","#define parameters. you do not need to edit this section\n","FromCondition = [\n","    (relationship_df['FromTableType'] == 'Fact'),\n","    (relationship_df['FromTableType'] == 'Dimension')\n","]\n","ToCondition = [\n","    (relationship_df['ToTableType'] == 'Fact'),\n","    (relationship_df['ToTableType'] == 'Dimension')\n","]\n","values = [factcolor,dimcolor]\n","###########################################################################################\n","relationship_df['ToColor'] = np.select(ToCondition,values,default=defaultcolor)\n","relationship_df['FromColor'] = np.select(FromCondition,values,default=defaultcolor)\n","#display(relationship_df)\n","\n","writeTable(relationship_df,\"Relationships\")\n","logActivity('Relationships','Relationship processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","session_id":"7f293312-2244-4a98-b187-e4258ae663d2","normalized_state":"finished","queued_time":"2025-11-26T19:43:23.0951787Z","session_start_time":null,"execution_start_time":"2025-11-26T19:46:46.5223213Z","execution_finish_time":"2025-11-26T19:47:00.8572339Z","parent_msg_id":"9694357e-d35c-43b4-a31c-7f9d3eb6af4f"},"text/plain":"StatementMeta(, 7f293312-2244-4a98-b187-e4258ae663d2, 21, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Relationships\",\"Relationship processing started\")\n"]},{"output_type":"stream","name":"stdout","text":["amaser_dp700.testthislakehouseoption.docuschema.Relationships\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Relationships\",\"Relationships written to lakehouse\")\n"]},{"output_type":"stream","name":"stdout","text":["log query=INSERT INTO amaser_dp700.testthislakehouseoption.docuschema.modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"Relationships\",\"Relationship processing completed\")\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"84c086e5-97c6-4fff-b450-4258bb25f4b8"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}