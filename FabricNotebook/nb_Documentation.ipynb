{"cells":[{"cell_type":"code","source":["%pip install semantic-link-labs"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"683fcea7-df9c-4620-a64a-36a0d1fa5652"},{"cell_type":"code","source":["#import necessary libraries\n","import sempy.fabric as fabric\n","import pandas as pd\n","import numpy as np\n","import re\n","from notebookutils import mssparkutils\n","\n","import sempy_labs as labs\n","from sempy_labs import directlake\n","from sempy_labs.tom import connect_semantic_model\n","\n","# set temporary config for date issue\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n","\n","##########################################################################################################\n","##                    THIS IS THE SECTION THAT REQUIRES INITIAL CONFIGURATION BY THE USER\n","##########################################################################################################\n","########################################################################################\n","###   MANUAL CONFIG HERE\n","#######################################################################################\n","#Define lakehouse and schema names\n","workspace_name = \"amaser_dp700\" #this is the workspace name where the notebook runs, and where the lakehouse lives\n","lakehouse_name = \"lh_ModelDocumentation\" #this is the name of the lakehouse, in the event that it needs to becreated by this notebook\n","lakehouse_description = \"Model Documentation Lakehouse\"  # this is only used if the notebook creates a new lakehouse\n","semantic_model_name = \"sem_ModelDocumentation\" ###NOTE: IF THIS MODEL ALREADY EXISTS, IT WILL BE OVERWRITTEN\n","model_type = \"DirectLake\"  # enter DirectLake if a DirectLake semantic model is desired.  Otherwise, enter Import\n","\n","# Define colors for table navigator page.  You don't need to adjust these, unless you want to rebrand the report\n","factcolor = '#A6B916'\n","dimcolor = '#C8B78A'\n","defaultcolor = '#A66999'\n","\n","#Create list of workspaces and Models.  This allows users to limit which models get documented.  A user could also\n","#   tweak this notebook, to use sempy functions, and query all workspaces/models that he/she has access to, if that's preferred.\n","#THE ORDER OF THE WORKSPACES AND MODELS MUST BE THE SAME\n","#  ie the third report MUST be in the third workspace\n","data = {\n","    'Workspace': ['Industry Demos', 'Industry Demos','PTurley AzureDevOps Integration'],\n","    'Model': ['Insurance Power BI Demo', 'Product Model','Airline Performance']\n","}\n","###########################################################################################\n","## from this section onward, there shouldn't be any updating necessary\n","#######################################################################################################\n","\n","workspace_id = fabric.get_workspace_id()\n","\n","#######################################################\n","## Check for existing items\n","######################################################\n","## check if lakehouse exists.  If not, create a lakehouse (non schema-enabled)\n","try:\n","    lakehouse_object = mssparkutils.lakehouse.get(lakehouse_name)\n","    lakehouse_id=lakehouse_object.id\n","except Exception as e:\n","    try:\n","        fabric.create_lakehouse(display_name=lakehouse_name,description=lakehouse_description,workspace=workspace_name,enable_schema=False)\n","        lakehouse_object = mssparkutils.lakehouse.get(lakehouse_name)\n","        lakehouse_id=lakehouse_object.id\n","    except Exception as e:\n","        print('Unable to create lakehouse')\n","        print(e)\n","\n","\n","lakehouse_value = f'{workspace_name}.{lakehouse_name}.'\n","lakehouse_value_full = f'{workspace_name}.{lakehouse_name}.dbo.'\n","\n","#print(lakehouse_value)\n","\n","#create the log table. \n","logquery = f'CREATE TABLE IF NOT EXISTS {lakehouse_value_full}modelLog (timeval TIMESTAMP, table_name STRING, Message STRING) USING DELTA'\n","#print(logquery)\n","spark.sql(logquery)\n","\n","\n","\n","####################################################################################################################\n","\n","\n","#DEFINE FUNCTIONS\n","#function to log activity.  This function can be tweaked to add lakehouse/schema name, so all the code in the lower cells don't need to be adjusted\n","def logActivity(table_name,message):\n","    logquery = f'INSERT INTO {lakehouse_value_full}modellog (timeval,table_name,Message) VALUES (CURRENT_TIMESTAMP(),\"{table_name}\",\"{message}\")'\n","    #print(f'log query={logquery}')\n","    #print(logquery)\n","    spark.sql(logquery)\n","\n","#function to write the dataframe as a database\n","def writeTable(pd_df, tablename):\n","  if len(pd_df) != 0:\n","    query = f\"drop table if exists {lakehouse_value}{tablename}\"\n","    spark.sql(query)\n","    spark_df = spark.createDataFrame(pd_df)\n","    full_table_name = f'{lakehouse_value}{tablename}'\n","    #print(full_table_name)\n","    try:\n","        spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").saveAsTable(full_table_name)\n","        logActivity(tablename,f'{tablename} written to lakehouse')\n","    except Exception as e:\n","        logActivity(tablename,f'Failed to write {tablename}')\n","  else:\n","    # if no data (ie no hierarchies exist) create a table so tahat the relationship code will work, and refreshes won't fail\n","    #   The page related to these will need to be hidden, in the event that one of the tables isnt used in any of your models\n","    strcreatequery = f'Create TABLE IF NOT EXCISTS {lakehouse_value_full}{tablename} (semanticmodel_id INTEGER {tablename}_name STRING)'\n","    spark.sql(strcreatequery)       \n","\n","#function to get data from model\n","def getMetaData(daxquery,fieldname,indexfield):\n","    indexname = indexfield+'_id'\n","    #build out first row\n","    model_df = fabric.evaluate_dax(\n","        workspace=df.iloc[0,0],\n","        dataset=df.iloc[0,1],\n","        dax_string=daxquery\n","    )\n","    model_df['semanticmodel_id'] = df.iloc[0,3]\n","    model_df['SemanticModel'] = df.iloc[0,1]\n","    #cycle through remaining rows, if more than 1 row\n","    if len(df) > 1:\n","        for index,row in df.iloc[1:].iterrows():\n","            temp_df = fabric.evaluate_dax(\n","                workspace=df.iloc[index,0],\n","                dataset=df.iloc[index,1],\n","                dax_string=dax_query\n","            )\n","            temp_df['semanticmodel_id']=df.iloc[index,3]\n","            temp_df['SemanticModel'] = df.iloc[index,1]\n","            #print('temp df executed')\n","            #append most recent\n","            model_df = pd.concat([model_df,temp_df],ignore_index=True)\n","\n","    model_df.columns = model_df.columns.str.replace('[', '')\n","    model_df.columns = model_df.columns.str.replace(']', '')\n","    model_df = model_df.rename(columns={'Name':fieldname})\n","    model_df[indexname] = model_df.index + 1\n","    return model_df\n","\n","\n","# Function to extract hashtags from a string\n","def extract_hashtags(description):\n","    if isinstance(description, str):  # Check if description is a string\n","        hashtags = re.findall(r'%%(\\w+)', description)\n","        return hashtags\n","    else:\n","        return []\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"03169fc2-a3a1-4141-aa03-d6f0e8feab77","normalized_state":"finished","queued_time":"2025-12-04T15:30:36.7985696Z","session_start_time":null,"execution_start_time":"2025-12-04T15:30:48.3278924Z","execution_finish_time":"2025-12-04T15:31:01.5632615Z","parent_msg_id":"c0fda24d-1529-4b3a-ab11-748a37a54f6b"},"text/plain":"StatementMeta(, 03169fc2-a3a1-4141-aa03-d6f0e8feab77, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":22,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ef28b2d-c188-4f5e-98c6-5dd1fb680be1"},{"cell_type":"markdown","source":["Process list of semantic models from the user-defined list. save as a table in the lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6b6e463c-50f6-4899-b592-deb0d7b667d8"},{"cell_type":"code","source":["# take the provided workspaces/models and convert it to a dataframe.  Add the full path as an extra column.\n","df = pd.DataFrame(data)\n","df['WorkspacePath'] = f'powerbi://api.powerbi.com/v1.0/myorg/'+ df['Workspace']\n","df['semanticmodel_id'] = df.index + 1 # addsan auto increment index to  the table.\n","logActivity('SemanticModel','SemanticModel processing started')\n","\n","writeTable(df,\"SemanticModel\")\n","logActivity('SemanticModel','SemanticModel processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":33,"statement_ids":[33],"state":"finished","livy_statement_state":"available","session_id":"bd373569-d4ba-4b33-b877-c89147495c09","normalized_state":"finished","queued_time":"2025-12-03T23:55:48.151527Z","session_start_time":null,"execution_start_time":"2025-12-03T23:55:48.1526919Z","execution_finish_time":"2025-12-03T23:55:58.3820549Z","parent_msg_id":"b01ec130-ab22-4898-84da-e3d6984879aa"},"text/plain":"StatementMeta(, bd373569-d4ba-4b33-b877-c89147495c09, 33, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"45b3f910-f26b-4ea8-a6d8-2ae8b6c29f1c"},{"cell_type":"markdown","source":["Process Model query.  Save Model data to the lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"04fff423-ad40-4ded-95dd-eb378d63a148"},{"cell_type":"code","source":["#Get Model Data\n","logActivity('Model','Model processing started')\n","dax_query = \"EVALUATE INFO.MODEL()\"\n","model_df = getMetaData(dax_query,'ModelName','model')\n","#display(model_df)\n","\n","writeTable(model_df,\"Model\")\n","logActivity('Model','Model processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":32,"statement_ids":[32],"state":"submitted","livy_statement_state":"running","session_id":"bd373569-d4ba-4b33-b877-c89147495c09","normalized_state":"running","queued_time":"2025-12-03T23:54:56.2069883Z","session_start_time":null,"execution_start_time":"2025-12-03T23:54:56.2082151Z","execution_finish_time":null,"parent_msg_id":"bf693bbe-56f2-4cba-a3b0-56236fbae5b4"},"text/plain":"StatementMeta(, bd373569-d4ba-4b33-b877-c89147495c09, 32, Submitted, Running, Running)"},"metadata":{}}],"execution_count":23,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"3b5072d7-6ed2-4097-8cb5-b5f46a3f6369"},{"cell_type":"markdown","source":["Process Tables dax query to collect table data from all defined models"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"03b0cb49-c861-4dfb-b7a8-01c421a234b3"},{"cell_type":"code","source":["#get Table Data\n","logActivity('PBITables','Table processing started')\n","dax_query = \"\"\"EVALUATE\n","VAR __tables =\n","    INFO.TABLES ()\n","VAR __storagemode =\n","    SELECTCOLUMNS ( INFO.VIEW.TABLES (), \\\"ID\\\", [ID], \\\"StorageMode\\\", [StorageMode] )\n","VAR __combined =\n","    NATURALLEFTOUTERJOIN ( __tables, __storagemode )\n","RETURN\n","    __combined\n","\n","\"\"\"\n","table_df = getMetaData(dax_query,'TableName','table')\n","\n","#add table type column for slicer. Cleanup column names and create combined key for joining\n","table_df['TableType']=table_df['Description'].apply(lambda x:extract_hashtags(x))\n","table_df['TableType']=table_df['TableType'].astype(str)\n","table_df['TableType'] = table_df['TableType'].str.removeprefix('[\\'')\n","table_df['TableType'] = table_df['TableType'].str.replace('\\']', '')\n","table_df['TableType'] = table_df['TableType'].str.replace(']', '')\n","table_df['TableType'] = table_df['TableType'].str.replace('[', '')\n","table_df['compoundkey'] = table_df['SemanticModel'] + table_df['ID'].astype(str)\n","\n","#display(table_df)\n","writeTable(table_df,\"PBITables\")\n","logActivity('PBITables','Table processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":34,"statement_ids":[34],"state":"finished","livy_statement_state":"available","session_id":"bd373569-d4ba-4b33-b877-c89147495c09","normalized_state":"finished","queued_time":"2025-12-03T23:56:46.4760266Z","session_start_time":null,"execution_start_time":"2025-12-03T23:56:46.4771194Z","execution_finish_time":"2025-12-03T23:57:01.0154788Z","parent_msg_id":"44e7034a-e6d7-4cbd-ada5-d55f704e73e1"},"text/plain":"StatementMeta(, bd373569-d4ba-4b33-b877-c89147495c09, 34, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"aa933b1d-5b64-408b-a671-c1279d629ad8"},{"cell_type":"markdown","source":["collect measure data from listed models and store in the lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"86a63b87-c467-4448-87f1-64686e26facd"},{"cell_type":"code","source":["#get measure Data\n","logActivity('MeasureList','MeasureList processing started')\n","dax_query = \"EVALUATE INFO.Measures()\"\n","measure_df = getMetaData(dax_query,'MeasureName','measures')\n","\n","#display(measure_df)\n","\n","writeTable(measure_df,\"MeasureList\")\n","logActivity('MeasureList','MeasureList processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":35,"statement_ids":[35],"state":"submitted","livy_statement_state":"running","session_id":"bd373569-d4ba-4b33-b877-c89147495c09","normalized_state":"running","queued_time":"2025-12-03T23:56:46.4843169Z","session_start_time":null,"execution_start_time":"2025-12-03T23:57:01.0175981Z","execution_finish_time":null,"parent_msg_id":"27fb3506-e866-4931-a6c7-5d736c32e8b6"},"text/plain":"StatementMeta(, bd373569-d4ba-4b33-b877-c89147495c09, 35, Submitted, Running, Running)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f4a2e9d8-f20c-49b4-8450-de7bae4505ef"},{"cell_type":"markdown","source":["\n","get role data (if any) from all selected models, and store it in the lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6e1d1ef0-bd78-493f-8b72-81ae2f3567b5"},{"cell_type":"code","source":["#get role Data\n","logActivity('PBIRoles','Role processing started')\n","dax_query = \"EVALUATE INFO.Roles()\"\n","role_df = getMetaData(dax_query,'RoleName','roles')\n","\n","#display(role_df)\n","\n","writeTable(role_df,\"PBIRoles\")\n","logActivity('PBIRoles','Role processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-12-03T23:56:46.4865628Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"2880a585-2c19-48f9-b8d1-72e18b797ad8"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"20b8978a-11fc-404a-b97e-c3c1abb9b9b5"},{"cell_type":"markdown","source":["get data about any partitions.  This may only be important in regards to incremental refresh, but either way, the data is available in the documentation lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0f6484c2-e4e4-4b68-bc7b-b52836e5a31d"},{"cell_type":"code","source":["#get partition Data\n","logActivity('Partitions','Partition processing started')\n","dax_query = \"\"\"\n","EVALUATE\n","VAR __partitions =\n","    INFO.PARTITIONS ()\n","VAR __refreshpolicies =\n","    SELECTCOLUMNS (\n","        INFO.REFRESHPOLICIES (),\n","        \\\"TableID\\\", [TableID],\n","        \\\"SourceExpression\\\", [SourceExpression]\n","    )\n","VAR __combined =\n","    NATURALLEFTOUTERJOIN ( __partitions, __refreshpolicies )\n","RETURN\n","    __combined\n","\"\"\"\n","partition_df = getMetaData(dax_query,'PartitionName','partitions')\n","\n","#add extra columns for joins, and cleanup unused columns\n","partition_df['compoundkey'] = partition_df['SemanticModel'] + partition_df['TableID'].astype(str)\n","partition_df =pd.merge(partition_df,table_df[['compoundkey','TableName']],left_on='compoundkey',right_on='compoundkey',how='left')\n","partition_df['Source'] = np.where(partition_df['QueryDefinition'].isnull(),partition_df['SourceExpression'],partition_df['QueryDefinition'])\n","partition_df = partition_df.drop(['QueryDefinition','SourceExpression'],axis=1)\n","#display(partition_df)\n","\n","writeTable(partition_df,\"Partitions\")\n","logActivity('Partitions','Partition processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-12-03T23:56:46.4886163Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"3e0d976e-5381-4982-9b05-fdc06d2f3388"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"82882fbe-a690-4440-9c8c-c89854bb0072"},{"cell_type":"markdown","source":["get expression data.  This includes data sources (as shown in TE) and any parameters that may be defined."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a72cb4aa-58f6-4ce6-a471-55b7fcc1d139"},{"cell_type":"code","source":["#get expression Data\n","logActivity('Expression','Expression processing started')\n","dax_query = \"EVALUATE INFO.Expressions()\"\n","expression_df = getMetaData(dax_query,'ExpressionName','expression')\n","\n","#display(expression_df)\n","\n","writeTable(expression_df,\"Expression\")\n","logActivity('Expression','Expression processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-12-03T23:56:46.4905175Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"14cb5606-30ab-4b07-a49f-29da2ed1c878"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b143325d-b5e3-4f54-a62c-a88ba4116204"},{"cell_type":"markdown","source":["get hierarchy data (if any)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4a32ab93-d97e-41c0-a608-2ecca7bc98b9"},{"cell_type":"code","source":["#get hierarchy Data\n","logActivity('Hierarchies','Hierarchy processing started')\n","dax_query = \"EVALUATE INFO.Hierarchies()\"\n","hierarchy_df = getMetaData(dax_query,'HierarchyName','hierarchy')\n","\n","#display(final_df)\n","#add colulmn for join with table\n","hierarchy_df['compoundkey'] = hierarchy_df['SemanticModel'] + hierarchy_df['TableID'].astype(str)\n","\n","hierarchy_df =pd.merge(hierarchy_df,table_df[['compoundkey','TableName']],left_on='compoundkey',right_on='compoundkey',how='left')\n","#display(hierarchy_df)\n","\n","writeTable(hierarchy_df,\"Hierarchies\")\n","logActivity('Hierarchies','Hierarchy processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-12-03T23:56:46.4925928Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"004fd6ed-c6a5-41bf-bb33-ed55a05eb70e"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8885ae53-93fb-4e70-ac87-e4cb89d0adb2"},{"cell_type":"markdown","source":["get column data from selected models"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e234bc3-a699-4187-a670-abc234001587"},{"cell_type":"code","source":["#get column Data\n","logActivity('PBIColumns','Column processing started')\n","dax_query = \"EVALUATE INFO.Columns()\"\n","columns_df = getMetaData(dax_query,'ColumnName','column')\n","\n","#add compound key for table join.  add colulmn to determine calculated columns (used for slicer)\n","columns_df['compoundkey'] = columns_df['SemanticModel'] + columns_df['TableID'].astype(str)\n","columns_df =pd.merge(columns_df,table_df[['compoundkey','TableName']],left_on='compoundkey',right_on='compoundkey',how='left')\n","columns_df['ColumnName'] = columns_df['ExplicitName'].combine_first(columns_df['InferredName'])\n","del columns_df['ExplicitName']\n","del columns_df['InferredName']\n","columns_df['ColumnType'] = np.where(columns_df['Expression'].isnull(),'Standard','Calculated')\n","#display(columns_df)\n","\n","writeTable(columns_df,\"PBIColumns\")\n","logActivity('PBIColumns','Column processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-12-03T23:56:46.4946015Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"29b83d2f-bb4d-4266-85ee-c4076a5681bb"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a9f7e173-a3ac-4188-a82c-80e2a7281ddb"},{"cell_type":"markdown","source":["get info on any calculation groups from selected models"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ba0eff92-dc61-478f-8d24-ac14ac50285e"},{"cell_type":"code","source":["#get calculation groups\n","logActivity('CalculationItems','Calculation Item processing started')\n","dax_query = \"EVALUATE INFO.CalculationGroups()\"\n","calcgroup_df = getMetaData(dax_query,'CalculationGroupName','calcgroup')\n","calcgroup_df['compoundkey'] = calcgroup_df['SemanticModel'] + calcgroup_df['TableID'].astype(str)\n","calcgroup_df['groupcompoundkey'] = calcgroup_df['SemanticModel'] + calcgroup_df['ID'].astype(str)\n","calcgroup_df =pd.merge(calcgroup_df,table_df[['compoundkey','TableName']],left_on='compoundkey',right_on='compoundkey',how='left')\n","calcgroup_df['CalculationGroupName'] = calcgroup_df['TableName']\n","#display(calcgroup_df)\n","\n","#get calcitems\n","dax_query = \"EVALUATE INFO.CalculationItems()\"\n","items_df = getMetaData(dax_query,'CalculationItemName','calcitem')\n","items_df['compoundkey'] = items_df['SemanticModel'] + items_df['CalculationGroupID'].astype(str)\n","items_df = pd.merge(items_df,calcgroup_df[['groupcompoundkey','CalculationGroupName']],left_on='compoundkey',right_on='groupcompoundkey',how='left')\n","#display(items_df)\n","\n","writeTable(items_df,\"CalculationItems\")\n","logActivity('CalculationItems','Calculation Item processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-12-03T23:56:46.4965413Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"0d7ce542-b010-4563-b354-ff5193f29b11"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e09de0ce-0815-4ccb-a341-6058dfd6da87"},{"cell_type":"markdown","source":["get Relationship data from selected models.  this does not include relationships for this documentation model "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9472f87c-5bcf-42a9-ade9-8e8a4c050fb3"},{"cell_type":"code","source":["#get relationships\n","logActivity('Relationships','Relationship processing started')\n","dax_query = \"EVALUATE INFO.Relationships()\"\n","relationship_df = getMetaData(dax_query,'RelationshipName','relationship')\n","# clean up colulmns for to/from relationships.  \n","relationship_df['FromCompoundKey'] = relationship_df['SemanticModel'] + relationship_df['FromTableID'].astype(str)\n","relationship_df['ToCompoundKey'] = relationship_df['SemanticModel'] + relationship_df['ToTableID'].astype(str)\n","relationship_df =pd.merge(relationship_df,table_df[['compoundkey','TableName','TableType']],left_on='FromCompoundKey',right_on='compoundkey',how='left')\n","relationship_df = relationship_df.rename(columns={'TableName':'FromTableName','TableType':'FromTableType'})\n","relationship_df =pd.merge(relationship_df,table_df[['compoundkey','TableName','TableType']],left_on='ToCompoundKey',right_on='compoundkey',how='left')\n","relationship_df = relationship_df.rename(columns={'TableName':'ToTableName','TableType':'ToTableType'})\n","relationship_df['FromCardinality'] = relationship_df['FromCardinality'].astype(str)\n","relationship_df['FromCardinality'] = relationship_df['FromCardinality'].replace('2','Many')\n","relationship_df['ToCardinality'] = relationship_df['ToCardinality'].astype(str)\n","relationship_df['ToCardinality'] = relationship_df['ToCardinality'].replace('2','Many')\n","####################################################################################################################\n","#define parameters. you do not need to edit this section\n","FromCondition = [\n","    (relationship_df['FromTableType'] == 'Fact'),\n","    (relationship_df['FromTableType'] == 'Dimension')\n","]\n","ToCondition = [\n","    (relationship_df['ToTableType'] == 'Fact'),\n","    (relationship_df['ToTableType'] == 'Dimension')\n","]\n","values = [factcolor,dimcolor]\n","###########################################################################################\n","relationship_df['ToColor'] = np.select(ToCondition,values,default=defaultcolor)\n","relationship_df['FromColor'] = np.select(FromCondition,values,default=defaultcolor)\n","#display(relationship_df)\n","\n","writeTable(relationship_df,\"Relationships\")\n","logActivity('Relationships','Relationship processing completed')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-12-03T23:56:46.4986563Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"d97ee0b5-66a0-4a1c-9ee5-af9d81796096"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"84c086e5-97c6-4fff-b450-4258bb25f4b8"},{"cell_type":"code","source":["if model_type == \"DirectLake\":\n","    ##Check if semantic model exists\n","    \n","    # List all datasets in the workspace\n","    # If workspace_name is not provided, it defaults to the notebook's attached lakehouse workspace or the notebook's own workspace.\n","    all_datasets = fabric.list_datasets(workspace=workspace_name if workspace_name in locals() else None)\n","\n","    # Check if the semantic model exists in the list\n","    if semantic_model_name in all_datasets['Dataset Name'].values:\n","        print(f\"The semantic model '{semantic_model_name}' exists in the workspace.\")\n","        print(\"THE SEMANTIC MODEL WILL BE REPLACED\")\n","        modelMessage = f'The semantic model {semantic_model_name} exists in the workspace and is being overwritten'\n","    else:\n","        print(f\"The semantic model '{semantic_model_name}' does not exist in the workspace.\")\n","        modelMessage = f'The semantic model {semantic_model_name} does not exist in the workspace.  It will be created'\n","\n","    logActivity(\"ModelCreation\",modelMessage) \n","    #add tables\n","\n","    table_list = [\"modellog\",\"calculationitems\",\"expression\",\"hierarchies\",\"measurelist\",\"model\",\"partitions\",\"pbicolumns\",\"pbiroles\",\"pbitables\",\"relationships\",\"semanticmodel\"]\n","\n","    directlake.generate_direct_lake_semantic_model(dataset=semantic_model_name,lakehouse_tables=table_list,workspace=workspace_name,lakehouse=lakehouse_name,lakehouse_workspace=workspace_name,schema='dbo',overwrite=True,refresh=True) \n","    df_datasets = fabric.list_datasets()\n","    semantic_model_id = df_datasets[df_datasets['Dataset Name'] == semantic_model_name]['Dataset ID'].iloc[0]\n","    logActivity(\"ModelCreation\",f\"Created Semantic Model {semantic_model_name}\")\n","    ##rename tables\n","    with fabric.connect_semantic_model(dataset=semantic_model_name, workspace=workspace_name, readonly=False) as tom:\n","        # Iterate through all tables in the model to find the target table\n","        for table in tom.model.Tables:\n","            if table.Name == 'expression':\n","                table.Name = 'Expressions'\n","            if table.Name == 'calculationitems':\n","                table.Name = 'Calculation Items'\n","            if table.Name == 'hierarchies':\n","                table.Name = 'Hierarchies'\n","            if table.Name == 'measurelist':\n","                table.Name = 'Measure List'\n","            if table.Name == 'model':\n","                table.Name = 'Models'\n","            if table.Name == 'partitions':\n","                table.Name = 'Partitions'\n","            if table.Name == 'pbicolumns':\n","                table.Name = 'Columns'\n","            if table.Name == 'pbiroles':\n","                table.Name = 'Roles'\n","            if table.Name == 'pbitables':\n","                table.Name = 'Tables'\n","            if table.Name == 'relationships':\n","                table.Name = 'Relationships'\n","            if table.Name == 'semanticmodel':\n","                table.Name = 'Semantic Models'\n","    \n","        print(\"Table rename completed\")\n","        logActivity(\"ModelCreation\",\"Table rename completed\")\n","\n","       \n","    #create relationships for directlake model\n","    with labs.tom.connect_semantic_model(dataset=semantic_model_name,\n","                            workspace=workspace_name,\n","                            readonly=False) as model:    \n","        \n","        model.add_relationship(\n","            to_table=\"Semantic Models\",\n","            to_column=\"semanticmodel_id\",\n","            from_table=\"Models\",\n","            from_column=\"semanticmodel_id\",\n","            from_cardinality=\"Many\",\n","            to_cardinality=\"One\",\n","            cross_filtering_behavior=\"Automatic\",\n","            is_active=True\n","        )\n","        model.add_relationship(\n","            to_table=\"Semantic Models\",\n","            to_column=\"semanticmodel_id\",\n","            from_table=\"Tables\",\n","            from_column=\"semanticmodel_id\",\n","            from_cardinality=\"Many\",\n","            to_cardinality=\"One\",\n","            cross_filtering_behavior=\"Automatic\",\n","            is_active=True\n","        )\n","        model.add_relationship(\n","            to_table=\"Tables\",\n","            to_column=\"compoundkey\",\n","            from_table=\"Columns\",\n","            from_column=\"compoundkey\",\n","            from_cardinality=\"Many\",\n","            to_cardinality=\"One\",\n","            cross_filtering_behavior=\"Automatic\",\n","            is_active=True\n","        )\n","        model.add_relationship(\n","            to_table=\"Semantic Models\",\n","            to_column=\"semanticmodel_id\",\n","            from_table=\"Calculation Items\",\n","            from_column=\"semanticmodel_id\",\n","            from_cardinality=\"Many\",\n","            to_cardinality=\"One\",\n","            cross_filtering_behavior=\"Automatic\",\n","            is_active=True\n","        )\n","        model.add_relationship(\n","            to_table=\"Semantic Models\",\n","            to_column=\"semanticmodel_id\",\n","            from_table=\"Expressions\",\n","            from_column=\"semanticmodel_id\",\n","            from_cardinality=\"Many\",\n","            to_cardinality=\"One\",\n","            cross_filtering_behavior=\"Automatic\",\n","            is_active=True\n","        )\n","        model.add_relationship(\n","            to_table=\"Semantic Models\",\n","            to_column=\"semanticmodel_id\",\n","            from_table=\"Measure List\",\n","            from_column=\"semanticmodel_id\",\n","            from_cardinality=\"Many\",\n","            to_cardinality=\"One\",\n","            cross_filtering_behavior=\"Automatic\",\n","            is_active=True\n","        )\n","        model.add_relationship(\n","            to_table=\"Semantic Models\",\n","            to_column=\"semanticmodel_id\",\n","            from_table=\"Hierarchies\",\n","            from_column=\"semanticmodel_id\",\n","            from_cardinality=\"Many\",\n","            to_cardinality=\"One\",\n","            cross_filtering_behavior=\"Automatic\",\n","            is_active=True\n","        )\n","        model.add_relationship(\n","            to_table=\"Semantic Models\",\n","            to_column=\"semanticmodel_id\",\n","            from_table=\"Partitions\",\n","            from_column=\"semanticmodel_id\",\n","            from_cardinality=\"Many\",\n","            to_cardinality=\"One\",\n","            cross_filtering_behavior=\"Automatic\",\n","            is_active=True\n","        )\n","        model.add_relationship(\n","            to_table=\"Semantic Models\",\n","            to_column=\"semanticmodel_id\",\n","            from_table=\"Roles\",\n","            from_column=\"semanticmodel_id\",\n","            from_cardinality=\"Many\",\n","            to_cardinality=\"One\",\n","            cross_filtering_behavior=\"Automatic\",\n","            is_active=True\n","        )\n","        model.add_relationship(\n","            to_table=\"Semantic Models\",\n","            to_column=\"semanticmodel_id\",\n","            from_table=\"Relationships\",\n","            from_column=\"semanticmodel_id\",\n","            from_cardinality=\"Many\",\n","            to_cardinality=\"One\",\n","            cross_filtering_behavior=\"Automatic\",\n","            is_active=True\n","        )\n","        model.add_table(name=f'_Measures',description=f'Table to hold any measures.%%Other',hidden=False)\n","        logActivity(\"_Measures\",\"Measures table created\")\n","        print(\"_Measures table created\")\n","        model.add_measure(\n","            table_name=f'_Measures',\n","            measure_name=f'Model Name',\n","            expression=f\"IF(COUNTROWS(VALUES('Semantic Models'[Model]))>1 , \\\"Multiple Models\\\",MAX('Semantic Models'[Model]))\",\n","            description=f'Measure to display selected model. Otherwise displays [Multiple Models]',\n","            hidden=False\n","        )\n","    print(\"Relationships added\")\n","    logActivity(\"ModelCreation\",\"Relationships Added\")\n","\n","    #final refresh\n","    #refresh the newly created dataset\n","    print(\"Dataset refresh started...\")\n","    fabric.refresh_dataset(workspace=workspace_name, dataset=semantic_model_name)\n","    print(\"Dataset refresh completed\")\n","    logActivity(\"ModelCreation\",\"Dataset refresh completed\")\n","    logActivity(\"ModelCreation\",\"Final refresh completed\")\n","    \n","else:\n","    print(\"Use the Import mode template and connect to the selected lakehouse for the report. \")\n","    logActivity(\"ModelCreation\",\"Import Mode selected.  No model created.\")\n","print('All activity completed!')\n","logActivity(\"FullProcess\",\"All Activity Completed\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"03169fc2-a3a1-4141-aa03-d6f0e8feab77","normalized_state":"finished","queued_time":"2025-12-04T15:37:49.2434484Z","session_start_time":null,"execution_start_time":"2025-12-04T15:37:49.2447703Z","execution_finish_time":"2025-12-04T15:39:12.7805671Z","parent_msg_id":"39423439-2be8-4ffc-9bc8-ba0219f4a1aa"},"text/plain":"StatementMeta(, 03169fc2-a3a1-4141-aa03-d6f0e8feab77, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The semantic model 'sem_ModelDocumentation' exists in the workspace.\nTHE SEMANTIC MODEL WILL BE REPLACED\n"]},{"output_type":"stream","name":"stdout","text":["ðŸŸ¢ The 'sem_ModelDocumentation' semantic model was created within the 'amaser_dp700' workspace.\n"]},{"output_type":"stream","name":"stdout","text":["âŒ› Refresh of the 'sem_ModelDocumentation' semantic model within the 'amaser_dp700' workspace is in progress...\n"]},{"output_type":"stream","name":"stdout","text":["ðŸŸ¢ Refresh 'full' of the 'sem_ModelDocumentation' semantic model within the 'amaser_dp700' workspace is complete.\n"]},{"output_type":"stream","name":"stdout","text":["Table rename completed\n"]},{"output_type":"stream","name":"stdout","text":["Relationships added\n"]},{"output_type":"stream","name":"stdout","text":["Dataset refresh started...\n"]},{"output_type":"stream","name":"stdout","text":["Dataset refresh completed\n"]}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"13c7b3ef-7c95-45d1-965c-a43415363808"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}